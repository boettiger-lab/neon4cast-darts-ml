{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9cf364-92af-4a2a-8792-a66946e538eb",
   "metadata": {},
   "source": [
    "# Using Historical mean and std as covariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4029428-b3d3-4a9c-ae26-c03b719be6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from residual_learning.utils import (\n",
    "                BaseForecaster, \n",
    "                ResidualForecasterDarts,\n",
    "                TimeSeriesPreprocessor,\n",
    "                crps,\n",
    "                HistoricalForecaster\n",
    ")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import smape, rho_risk\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Optional\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape\n",
    "from darts.models import (\n",
    "                          BlockRNNModel, \n",
    "                          TCNModel, \n",
    "                          RNNModel, \n",
    "                          TransformerModel, \n",
    "                          NLinearModel,\n",
    "                          DLinearModel,\n",
    "                          NBEATSModel,\n",
    "                          XGBModel,\n",
    "                          LinearRegressionModel,\n",
    "                          TFTModel,\n",
    "                         )\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import CRPS.CRPS as forecastscore\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "targets = pd.read_csv(\"targets.csv.gz\")\n",
    "\n",
    "data_preprocessor = TimeSeriesPreprocessor()\n",
    "data_preprocessor.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb6c238-1d6a-4c20-96c8-8dd6762f76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(csv_name, site, target_variable, data_preprocessor, record_dict):\n",
    "    # Loading the forecast csv and creating a time series\n",
    "    df = pd.read_csv(f\"{csv_name}.csv\")\n",
    "    times = pd.to_datetime(df[\"datetime\"])\n",
    "    times = pd.DatetimeIndex(times)\n",
    "    values = df.loc[:, df.columns!=\"datetime\"].to_numpy().reshape((len(times), 1, -1))\n",
    "    model_forecast = TimeSeries.from_times_and_values(times, \n",
    "                                                      values, \n",
    "                                                      fill_missing_dates=True, freq=\"D\")\n",
    "\n",
    "    # Presuming data_preprocessor has been called outside of the function\n",
    "    # (not the best practice), create a validation series from it\n",
    "    target_series = data_preprocessor.sites_dict[site][target_variable]\n",
    "    validation_series = target_series.slice(times[0], times[-1]).median()\n",
    "\n",
    "    # Now, making the forecast based off of historical mean and std\n",
    "    historical_model = HistoricalForecaster(data_preprocessor=data_preprocessor,\n",
    "                          output_csv_name=\"historical_forecaster_output.csv\",\n",
    "                          validation_split_date=str(model_forecast.time_index[0])[:10],\n",
    "                          forecast_horizon=30,\n",
    "                          site_id=site,\n",
    "                          target_variable=target_variable)\n",
    "    historical_model.make_forecasts()\n",
    "    \n",
    "    if site not in record_dict.keys():\n",
    "        record_dict[site] = {target_variable: {csv_name: model_forecast}}\n",
    "        record_dict[site][target_variable][\"validation\"] = validation_series\n",
    "        # I need to make draw samples from historical would be best to just do this in utils\n",
    "        record_dict[site][target_variable][\"historical\"] = historical_model.forecast_ts\n",
    "    else:\n",
    "        record_dict[site][target_variable][csv_name] = model_forecast    \n",
    "    \n",
    "    validation_series.plot(label=\"Truth\")\n",
    "    model_forecast.plot(label=\"Forecast\")\n",
    "    historical_model.forecast_ts.plot(label=\"historical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051eb42a-c30c-492d-95f9-348cdf09bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseForecasterHistPlus():\n",
    "    def __init__(self,\n",
    "                 model: Optional[str] = None,\n",
    "                 data_preprocessor: Optional = None,\n",
    "                 target_variable_column_name: Optional[str] = None,\n",
    "                 datetime_column_name: Optional[str] = None,\n",
    "                 covariates_names: Optional[list] = None,\n",
    "                 output_csv_name: Optional[str] = \"residual_forecaster_output.csv\",\n",
    "                 validation_split_date: Optional[str] = None, #YYYY-MM-DD\n",
    "                 model_hyperparameters: Optional[dict] = None,\n",
    "                 model_likelihood: Optional[dict] = None,\n",
    "                 forecast_horizon: Optional[int] = 30,\n",
    "                 site_id: Optional[str] = None,\n",
    "                 historical_covariates: Optional[bool] = False,\n",
    "                 ):\n",
    "        self.model_ = {\"BlockRNN\": BlockRNNModel, \n",
    "                       \"TCN\": TCNModel, \n",
    "                       \"RNN\": RNNModel, \n",
    "                       \"Transformer\": TransformerModel,\n",
    "                       \"NLinear\": NLinearModel,\n",
    "                       \"DLinear\": DLinearModel,\n",
    "                       \"XGB\": XGBModel,\n",
    "                       \"NBEATS\": NBEATSModel,\n",
    "                       \"Linear\": LinearRegressionModel,\n",
    "                       \"TFT\": TFTModel}[model]\n",
    "        self.data_preprocessor = data_preprocessor\n",
    "        self.target_variable_column_name = target_variable_column_name\n",
    "        self.datetime_column_name = datetime_column_name\n",
    "        self.covariates_names = covariates_names\n",
    "        self.output_csv_name = output_csv_name\n",
    "        self.validation_split_date = validation_split_date\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.site_id = site_id\n",
    "        if model_hyperparameters == None:\n",
    "            self.hyperparams = {\"input_chunk_length\" : 180}\n",
    "        else:\n",
    "            self.hyperparams = model_hyperparameters\n",
    "        self.model_likelihood = model_likelihood\n",
    "        self.historical_covariates = historical_covariates\n",
    "\n",
    "        self._preprocess_data()\n",
    "        \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Performs gap filling and processing of data into format that\n",
    "        Darts models will accept\n",
    "        \"\"\"\n",
    "        stitched_series_dict = self.data_preprocessor.sites_dict[self.site_id]\n",
    "\n",
    "        # If there was failure when doing the GP fit then we can't do preprocessing\n",
    "        if self.target_variable_column_name in \\\n",
    "                self.data_preprocessor.sites_dict_null[self.site_id]:\n",
    "            return \"Cannot fit this target time series as no GP fit was performed.\"\n",
    "        self.inputs = stitched_series_dict[self.target_variable_column_name]\n",
    "\n",
    "        # And not using the covariates that did not yield GP fits beforehand\n",
    "        for null_variable in self.data_preprocessor.sites_dict_null[self.site_id]:\n",
    "            self.covariates_names.remove(null_variable)\n",
    "            \n",
    "        # Initializing covariates list then concatenating in for loop\n",
    "        self.covariates = stitched_series_dict[self.covariates_names[0]]\n",
    "        for cov_var in self.covariates_names[1:]:\n",
    "            self.covariates = self.covariates.concatenate(stitched_series_dict[cov_var], \n",
    "                                                          axis=1, \n",
    "                                                          ignore_time_axis=True)\n",
    "        \n",
    "        # Splitting training and validation set\n",
    "        year = int(self.validation_split_date[:4])\n",
    "        month = int(self.validation_split_date[5:7])\n",
    "        day = int(self.validation_split_date[8:])\n",
    "        split_date = pd.Timestamp(year=year, month=month, day=day)\n",
    "        self.training_set, self.validation_set = self.inputs.split_before(split_date)\n",
    "        if self.historical_covariates:\n",
    "            self.covariates, _ = self.covariates.split_before(split_date)\n",
    "            self.get_historical_df(self.training_set)\n",
    "            self.covariates = self.covariates.concatenate(self.historical_samples,\n",
    "                                                          axis=1,\n",
    "                                                          ignore_time_axis=True)\n",
    "\n",
    "    def get_historical_df(self, time_series):\n",
    "        # Using the medians of the time series\n",
    "        median_df = time_series.median().pd_dataframe()\n",
    "        median_df[\"timestamp\"] = pd.to_datetime(median_df.index)\n",
    "        median_df[\"day_of_year\"] = median_df[\"timestamp\"].dt.dayofyear\n",
    "        self.median_df = median_df\n",
    "        \n",
    "        # Computing average and std for doy's \n",
    "        self.doy_df = self.median_df.groupby(['day_of_year'])['0'].agg(['mean', 'std'])\n",
    "        self.doy_df = self.doy_df.fillna(method=\"ffill\")\n",
    "        # This produces a new df that gives mean and std for every date (indexed by doy)\n",
    "        # from the original TimeSeries\n",
    "        doy_hist_df = self.doy_df.loc[time_series.time_index.dayofyear]\n",
    "\n",
    "        # Now drawing samples so that we can create a Time Series\n",
    "        samples = np.array([np.random.normal(doy_hist_df.iloc[i][\"mean\"], \n",
    "                                             doy_hist_df.iloc[i][\"std\"]/2, \n",
    "                                             size=(1,500))\n",
    "                            for i in range(len(doy_hist_df))])\n",
    "        # Reconstituting a TimeSeries\n",
    "        self.historical_samples = TimeSeries.from_times_and_values(\n",
    "                                                   time_series.time_index,\n",
    "                                                   samples)\n",
    "\n",
    "    def tune(self,\n",
    "             hyperparameter_dict: Optional[dict]\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Sets up Optuna trial to perform hyperparameter tuning\n",
    "        Input dictionary will be of the form {\"hyperparamter\": [values to be tested]}\n",
    "        \"\"\"\n",
    "        # Setting up an optuna Trial\n",
    "        def objective(trial):\n",
    "            callback = [PyTorchLightningPruningCallback(trial, monitor=\"val_loss\")]\n",
    "            hyperparams = {key: trial.suggest_categorical(key, value) \n",
    "                                               for key, value in hyperparameter_dict.items()}\n",
    "        \n",
    "            model = self.model_(**hyperparams,\n",
    "                                output_chunk_length=self.forecast_horizon,\n",
    "                                **self.model_likelihood)\n",
    "            \n",
    "            self.scaler = Scaler()\n",
    "            training_set, covariates = self.scaler.fit_transform([self.training_set,\n",
    "                                                                  self.covariates])\n",
    "            # Don't need to tune XGB and linear regression models\n",
    "            extras = {\"past_covariates\": covariates,\n",
    "                      \"verbose\": False,\n",
    "                      \"epochs\": 500}\n",
    "        \n",
    "            model.fit(training_set,\n",
    "                           **extras)\n",
    "        \n",
    "            predictions = model.predict(n=len(self.validation_set[:self.forecast_horizon]), \n",
    "                                            past_covariates=covariates, \n",
    "                                            num_samples=50)\n",
    "            predictions = self.scaler.inverse_transform(predictions)\n",
    "            smapes = smape(self.validation_set[:self.forecast_horizon], \n",
    "                           predictions, \n",
    "                           n_jobs=-1, \n",
    "                           verbose=False)\n",
    "            \n",
    "            smape_val = np.mean(smapes)\n",
    "            return smape_val if smape_val != np.nan else float(\"inf\")\n",
    "\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        \n",
    "        study.optimize(objective, n_trials=50) # Note 10 trials pretty meaningless here\n",
    "        \n",
    "        self.hyperparams = study.best_trial.params\n",
    "\n",
    "    def make_forecasts(self):\n",
    "        \"\"\"\n",
    "        This function fits a Darts model to the training_set\n",
    "        \"\"\"\n",
    "        print(self.hyperparams)\n",
    "        self.model = self.model_(**self.hyperparams,\n",
    "                                 output_chunk_length=self.forecast_horizon,\n",
    "                                 **self.model_likelihood,\n",
    "                                 random_state=0)\n",
    "        self.scaler = Scaler()\n",
    "        training_set, covariates = self.scaler.fit_transform([self.training_set,\n",
    "                                                              self.covariates])\n",
    "        # Need to treat XGB and Linear Regression models differently than networks\n",
    "        extras = {\"past_covariates\": covariates,\n",
    "                  \"verbose\": False,\n",
    "                  \"epochs\": 500}\n",
    "        if self.model_ == XGBModel or self.model_ == LinearRegressionModel:\n",
    "            del extras[\"epochs\"]\n",
    "            del extras[\"verbose\"]\n",
    "        #import pdb; pdb.set_trace()\n",
    "        extras[\"past_covariates\"] = extras[\"past_covariates\"].median()\n",
    "        self.model.fit(training_set.median(),\n",
    "                       **extras)\n",
    "\n",
    "        predictions = self.model.predict(n=self.forecast_horizon,\n",
    "                                         past_covariates=covariates.median(), \n",
    "                                         num_samples=500)\n",
    "        predictions = self.scaler.inverse_transform(predictions)\n",
    "\n",
    "        predictions.pd_dataframe().to_csv(self.output_csv_name)\n",
    "\n",
    "    def get_historicals_and_residuals(self):\n",
    "        \"\"\"\n",
    "        This function creates a historical forecast along with their residual errors \n",
    "        \"\"\"\n",
    "        # This presumes that the scaler will not have been modified in interim \n",
    "        # from calling `make_forecasts`\n",
    "        training_set, covariates = self.scaler.transform([self.training_set,\n",
    "                                                              self.covariates])\n",
    "        historical_forecasts = self.model.historical_forecasts(\n",
    "                                            series=training_set,\n",
    "                                            past_covariates=covariates,\n",
    "                                            num_samples=500,\n",
    "                                            forecast_horizon=self.forecast_horizon,\n",
    "                                            stride=self.forecast_horizon,\n",
    "                                            retrain=False,\n",
    "                                            last_points_only=False\n",
    "                                            )\n",
    "        historical_forecasts = [self.scaler.inverse_transform(historical_forecast) for\n",
    "                                                historical_forecast in historical_forecasts]\n",
    "        # Getting the target time series slice for the historical forecast\n",
    "        self.historical_ground_truth = self.training_set.slice(\n",
    "                                            historical_forecasts[0].time_index[0], \n",
    "                                            historical_forecasts[-1].time_index[-1])\n",
    "\n",
    "        # Now concatenating the historical forecasts which were returned\n",
    "        # as a list above\n",
    "        self.historical_forecasts = historical_forecasts[0]\n",
    "        for time_series in historical_forecasts[1:]:\n",
    "            self.historical_forecasts = self.historical_forecasts.concatenate(time_series, \n",
    "                                                                axis=0, \n",
    "                                                                ignore_time_axis=True)\n",
    "\n",
    "        self.residuals = self.historical_ground_truth - self.historical_forecasts\n",
    "\n",
    "    def make_residuals_csv(self):\n",
    "        covariates_df = self.covariates.pd_dataframe()\n",
    "        forecast_df = self.historical_forecasts.pd_dataframe()\n",
    "        observed_df = self.historical_ground_truth.pd_dataframe()\n",
    "        residuals_df = self.residuals.pd_dataframe()\n",
    "\n",
    "        # Creating a folder if it doesn't exist already\n",
    "        if not os.path.exists(f\"{self.model}_residuals/\"):\n",
    "            os.makedirs(f\"{self.model}_residuals/\")\n",
    "        # Saving csv's in the **model name**_test directory\n",
    "        df_dict = {\"covariates\": block_rnn_forecaster.covariates.pd_dataframe(),\n",
    "                   \"forecast\": block_rnn_forecaster.historical_forecasts.pd_dataframe(),\n",
    "                   \"observed\": block_rnn_forecaster.historical_ground_truth.pd_dataframe()}\n",
    "        for variable, df in df_dict.items():\n",
    "            df.to_csv(f\"{self.model}_test/{variable}\")\n",
    "\n",
    "    def plot_by_site(self, site):\n",
    "        for key in self.sites_dict[site].keys():\n",
    "            plt.clf()\n",
    "            self.sites_dict[site][key].plot(color=\"blue\", label=f\"{key} @ {site}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a11025a-33cb-45a6-8428-0b1da19e5f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_chunk_length': 540, 'hidden_dim': 16, 'model': 'GRU', 'n_rnn_layers': 4, 'add_encoders': {'datetime_attribute': {'past': ['dayofyear']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "`Trainer.fit` stopped: `max_epochs=500` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9ed1f12ab8475186c495335b892269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are transforming a stochastic TimeSeries (i.e., contains several samples). The resulting DataFrame is a 2D object with all samples on the columns. If this is not the expected behavior consider calling a function adapted to stochastic TimeSeries like quantile_df().\n"
     ]
    }
   ],
   "source": [
    "forecaster_hist_test = BaseForecasterHistPlus(model=\"BlockRNN\",\n",
    "                                      data_preprocessor=data_preprocessor,\n",
    "                                      target_variable_column_name=\"oxygen\",\n",
    "                                      datetime_column_name=\"datetime\",\n",
    "                                      covariates_names=[\"air_tmp\", \"chla\", \"temperature\"],\n",
    "                                      output_csv_name=\"trash_block_test.csv\",\n",
    "                                      validation_split_date=\"2023-03-01\",\n",
    "                                      model_hyperparameters={'input_chunk_length': 540, \n",
    "                                                             'hidden_dim': 16, \n",
    "                                                             'model': 'GRU', \n",
    "                                                             'n_rnn_layers': 4,\n",
    "                                                             'add_encoders': {'datetime_attribute': {'past': ['dayofyear']}}},\n",
    "                                      model_likelihood={\"likelihood\": QuantileRegression([0.05, 0.1, 0.5, 0.9, 0.95])},\n",
    "                                      forecast_horizon=30,\n",
    "                                      site_id=\"ARIK\",\n",
    "                                      historical_covariates=True,)\n",
    "forecaster_hist_test.make_forecasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb8603-4156-436f-862c-b983c37c62ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
