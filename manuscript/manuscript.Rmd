---
title: "A Comparison of Neural Network Models for Water Quality Forecasting"
authors:
  - name: Marcus Lapeyrolerie
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: mlapeyro@berkeley.edu
  - name: Carl Boettiger
    department: Department of Environmental Science, Policy, and Management
    affiliation: University of California, Berkeley
    location: Berkeley, California
    email: cboettig@berkeley.edu (corresponding author)
abstract: |
  1. 
  2. 
  3. 
  4. 
  
keywords:
  - Artificial Intelligence
  - Forecasting
  - Machine Learning
  - Time Series
  - Limnology

bibliography: references.bib
nocite: |
    
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{lineno}
  - \usepackage{bbm}
  - \usepackage{setspace}
  - \usepackage{graphicx}
  - \usepackage{subcaption}
  - \linenumbers
  - \doublespacing
  
output: 
  rticles::arxiv_article:
    keep_tex: true
  word_document: default
editor_options: 
  markdown: 
    wrap: sentence
---
\setlength{\parindent}{15pt}

# Introduction

The ability to accurately forecast water quality variables has become increasingly important in the era of global change. Freshwater ecosystems have been disproportionately impacted by anthropogenic activities, a trend that is expected to continue in the future \cite{albert2021}. Since water quality variables, like water temperature, dissolved oxygen and chlorophyll concentration, effect many biological and physical processes in freshwater ecosystems \cite{oullet-proulx, Stajkowski, Chen}, preemptive water quality forecasts could allow managers to evade situations with far-reaching negative consequences \cite{}. Limnologists have historically used statistical, process-based and machine learning models to forecast water quality metrics \cite{maier}; but, in recent years, researchers have shown that machine learning models are particularly well suited to take advantage of recent advancements in computer science and the rising availability of water quality data  \cite{Hanson, zwart}. A shortcoming of past limnological forecasting studies that support the use of machine learning is that they tend to focus on a selection of sites that are not representative of freshwater systems across a broad geographic scale. This paper has the primary aim of making a comprehensive comparison of state-of-the-art machine learning methods by evaluating forecasts at 34 different sites across North America at different times of the year.

The time series that are used in this study are taken from the National Ecological Observatory Network's (NEON) Ecological Forecasting Challenge, an open challenge where teams can submit forecasts to data that is collected and made publicly accessible by NEON \cite{Quinn}. A common finding across the challenge is that a day of year historical mean model (also referred to as the climatology model) commonly produces top scoring forecasts \cite{Dietze, Quinn}. For instance, in a model comparison that examined the forecasts for phenological time series, \cite{Dietze} found that the climatology model outperformed all except one of the submitted models; and, the best performing model only marginally outperformed the climatology model. Thus, a primary consideration in this paper is comparing the performance of the machine learning models against the historical null model.

There are many promising neural network architectures that have not been evaluated for water quality forecasting. Most applications of neural networks to limnological time series have focused on the long-short-term-memory (LSTM) network which was published in 1997 \cite{lstm}. Over the nearly 30 years since LSTM's were introduced, researchers have identified that LSTM's have notable shortcomings, and there have been continued efforts towards the development of better performing models. Many recent studies that use neural networks for water quality forecasting have not explored more contemporary neural network architectures. We will address this gap in the literature by comparing 8 neural network models including LSTM's as well as more recently published neural network models.

# Materials and Methods

We evaluated the forecasting performance of 11 different models on water temperature, dissolved oxygen and chlorophyll-A time series, which were collected by in-situ sensors at 34 freshwater sites across North America. These sites consist of Lakes, Non-wadeable Rivers and Wadeable Streams, subtypes classified by NEON. There is variability across sites in which target variables were observed. Additionally, maintenance issues led to gaps in the data which also varied with the site. Since the training data spanned less than 3 years, we excluded time series that had gaps longer than a year. Provided the lack of time series and large gaps at certain sites, we evaluated the forecast performance for water temperature, dissolved oxygen and chlorophyll-A at 33, 32 and 9 sites, respectively. 

The imputation of missing data proved to be critical to the performance of the ML models. After experimenting with data filling methods that resulted in poor model performance, we developed an imputation method inspired by the climatology model. If the gap size was less than 5 days, then the gap was filled using a Gaussian Process Filter. For gaps over 5 days, missing data was estimated using the daily historical mean when this statistic is available; and, for the case when there are no data collected for a day of the year, either the monthly median, quarterly median, previous observation or global median was used in this order of preference.

We compared the performance of the neural network models to the climatology and naive persistence model. The climatology model generates forecasts by finding the daily mean and standard deviation and draws samples from a Gaussian Distribution with these parameters. The naive persistence model finds the last observed value of the target variable and predicts this value for each day in the forecast horizon. For each model, we computed the Continuous Ranked Probability Score (CRPS) and the Root Mean Square Error (RMSE). To gauge how models performed in relation to the climatology model, we computed the Continuous Ranked Probability Skill Score (CRPSS) which we defined as 
\begin{equation}
CRPSS_{model} = 1 - \frac{CRPS_{model}}{CRPS_{clim.}}.
\end{equation}

And using the naive persistence model as a reference, we computed the RMSE Skill Score,

\begin{equation}
RMSE-SS_{model} = 1 - \frac{RMSE_{model}}{RMSE_{naive.}}.
\end{equation}

If a target model outperforms than the reference model, then the target model will have a positive skill score. If the target model performs worse than the reference model, the skill score will be negative.

Talk about theta model

For the 8 neural network models that were investigated, there were varying design choices made regarding which type of covariates could be used. For instance, models could accept past covariates only, future covariates only or both future and past covariates. 

For the models that accept past covariates (**list these models**), we used the other target variables recorded at that site as well as air temperature as covariates. For the models that only accept future covariates (**list these models**), we used the day of the year as the covariate. The historical time series were split into a training and a validation set, whereby the models were trained on time series from 2020 to 2023 and validated at 12 30-day non-overlapping intervals in 2023. 


We provide fully reproducible code for fitting, scoring and visualizing the forecasts. All the machine learning forecasts were generated using the `darts` python library. `darts` is similar to other libraries `scikit-learn` in Python or `tidymodels` in R that allow users to easily employ time series forecasting models without having to implement them. For model comparison, we rely on the `CRPS` library in Python to efficiently compute the continuous ranked probability scores (CRPS). When comparing the probabilistic forecasts, we express the CRPS score in error-orientation -- i.e., a lower CRPS indicates a smaller margin of error.

## Machine Learning Models

All the machine learning algorithms that are used in this study are based on neural network architectures. Neural networks have the property of being universal function approximators, so theoretically, it is possible that all of these models could exactly approximate the data generating process \cite{}. Yet, it is often true that neural networks greatly underperform their function approximation capabilities in practice \cite{adcock, makridakis}. This performance gap can be due to a variety of reasons including overfitting and insufficient hyperparameter tuning \cite{adcock}. The discrepancy between theory and practice in function approximation with neural networks motivates research on how machine learning models perform in specific domains.

The 8 machine learning models that we compare take a variety of approaches with the design of their neural networks. We will not go into detail on how these different models work, but for those interested to learn more, we list the models and their references in table \ref{}. For readers who do not specialize in ML, an important concept to understand is that the ML models learn directly from the data and are not instilled with domain knowledge. This non-mechanisitic basis is at once very powerful as it does not restrict the models with misleading assumptions but is simultaneously limiting as these models are not readily interpretable and generally require more data than mechanistic analogs \cite{}. For the management of critical resources like water, this lack of interpretability could be a deterrent to the use of ML methods, and there may not be enough data to accomodate ML approaches \cite{}.

The ML models are configured to generate probabilistic forecasts. The neural network architectures that are used in this study are deterministic, so they are not able to directly represent probability distributions. We work around this limitation by performing quantile regression whereby the neural networks are trained to output quantiles at each time step in the forecast window. Forecasts are then generated by drawing samples according to these quantiles.

# Results

\begin{table}[!htbp]
    \centering
    \caption{Mean CRPS and RMSE}
    \begin{subtable}{\textwidth}
        \centering
        \caption{Dissolved Oxygen}
        \begin{tabular}{lcc}
            \toprule
            Model & Mean CRPS & Mean RMSE \\
            \midrule
            Climatology & 0.62 & 0.92 \\
            Naive Persistence & & 1.55 \\
            AutoTheta & 0.67 & 1.02 \\
            BlockRNN & 0.52 & 0.80 \\
            DLinear & 0.52 & 0.81  \\
            NBEATS & 0.51 & 0.80 \\
            NLinear & 0.52 & 0.80 \\
            \textbf{NaiveEnsemble} & \textbf{0.47} & \textbf{0.74} \\
            RNN & 0.60 & 0.92 \\
            TCN & 0.62 & 0.98 \\
            TFT & 0.52 & 0.80 \\
            Transformer & 0.51 & 0.80 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \centering
        \caption{Water Temperature}
        \begin{tabular}{lcc}
            \toprule
            Model & Mean CRPS & Mean RMSE \\
            \midrule
            Climatology & 1.46 & 2.19 \\
            Naive Persistence & & 6.05 \\
            AutoTheta & 1.55 & 2.35 \\
            BlockRNN & 1.45 & 2.26 \\
            DLinear & 1.35 & 2.08 \\
            NBEATS & 1.36 & 2.10 \\
            NLinear & 1.33 & 2.06 \\
            \textbf{NaiveEnsemble} & \textbf{1.18} & \textbf{1.80} \\
            RNN & 1.32 & 2.03 \\
            TCN & 1.89 & 3.02 \\
            TFT & 1.19 & 1.86 \\
            Transformer & 1.32 & 2.05 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \centering
        \caption{Chlorophyll-a}
        \begin{tabular}{lcc}
            \toprule
            Model & Mean CRPS & Mean RMSE \\
            \midrule
            Climatology & 4.83 & 7.50 \\
            \textbf{Naive Persistence} & & \textbf{4.86} \\
            AutoTheta &  4.20 & 6.31 \\
            \textbf{BlockRNN} & \textbf{3.14} & 5.16 \\
            DLinear & 3.47 & 5.48 \\
            NBEATS & 3.78 & 5.95 \\
            NLinear & 3.67 & 5.63 \\
            NaiveEnsemble & 3.44 & 5.50 \\
            RNN & 4.15 & 6.40 \\
            TCN & 3.72 & 5.77 \\
            TFT & 3.67 & 5.87 \\
            Transformer & 4.02 & 6.35 \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    \label{tab:mean_crps_rmse}
\end{table}


\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/sitetype_legend_0.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/type_local_historical_oxygen.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/type_local_naive_oxygen.png}
  \end{subfigure}
  \caption{Comparison of Historical and Naive Oxygen}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/sitetype_legend_0.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/type_local_historical_temperature.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/type_local_naive_temperature.png}
  \end{subfigure}
  \caption{Comparison of Historical and Naive temperature}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/sitetype_legend_1.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/type_local_historical_chla.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/type_local_naive_chla.png}
  \end{subfigure}
  \caption{Comparison of Historical and Naive chla}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_historical_oxygen.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_naive_oxygen.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \rule{0pt}{0pt}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_historical_temperature.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_naive_temperature.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_legend.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_historical_chla.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/intra_naive_chla.png}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \rule{0pt}{0pt}
  \end{subfigure}
  
  \caption{Oxygen, Temperature, Chlorophyll Intrawindow plots}
\end{figure}




\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/forecast_legend.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/oxygen_theta.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/chla_theta.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/oxygen_tft.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/chla_tft.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/oxygen_ensemble.png}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../plots/chla_ensemble.png}
  \end{subfigure}
  \caption{Individual Forecast Examples}
\end{figure}



# Discussion


Over the last decade, ML has gained significant prominence after a spate of high profile successes like beating the master champion of Go, accurately predicting protein structures and the advent of large language models like ChatGPT \cite{alphago, alphafold, chatgpt}. During this time, researchers have developed many ML models for time series forecasting with promises of significant accuracy improvements over previous methods \cite{makridakis}. For this study, we selected 8 ML models that include s

Yet, these claims of improvement tend to have major limitations, notably that the studies examine a limited amount of time series and they do not compare to naive benchmarks \cite{makridakis}. 

Models tend to struggle to accurately forecast chlorophyll during bloom conditions \cite{woelmer}. Accurately predicting timing and magnitude of bloom events is not surprising

# Acknowledgements


# Conflicts of Interest {-}

The authors declare there are no conflicts of interest.

# Authors' Contributions {-}

Marcus Lapeyrolerie and Carl Boettiger developed the code and wrote the manuscript.

# Data Availability {-}



\newpage

# References
