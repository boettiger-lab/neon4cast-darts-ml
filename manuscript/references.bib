
@article{albert2021,
	title = {Scientists’ warning to humanity on the freshwater biodiversity crisis},
	volume = {50},
	issn = {0044-7447, 1654-7209},
	url = {http://link.springer.com/10.1007/s13280-020-01318-8},
	doi = {10.1007/s13280-020-01318-8},
	language = {en},
	number = {1},
	urldate = {2024-05-21},
	journal = {Ambio},
	author = {Albert, James S. and Destouni, Georgia and Duke-Sylvester, Scott M. and Magurran, Anne E. and Oberdorff, Thierry and Reis, Roberto E. and Winemiller, Kirk O. and Ripple, William J.},
	month = jan,
	year = {2021},
	pages = {85--94},
	file = {Full Text:/Users/marcus/Zotero/storage/RWHACJ3X/Albert et al. - 2021 - Scientists’ warning to humanity on the freshwater .pdf:application/pdf},
}


@article{ouellet-proulx,
	title = {Water {Temperature} {Ensemble} {Forecasts}: {Implementation} {Using} the {CEQUEAU} {Model} on {Two} {Contrasted} {River} {Systems}},
	volume = {9},
	issn = {2073-4441},
	shorttitle = {Water {Temperature} {Ensemble} {Forecasts}},
	url = {http://www.mdpi.com/2073-4441/9/7/457},
	doi = {10.3390/w9070457},
	language = {en},
	number = {7},
	urldate = {2024-02-15},
	journal = {Water},
	author = {Ouellet-Proulx, Sébastien and St-Hilaire, André and Boucher, Marie-Amélie},
	month = jun,
	year = {2017},
	pages = {457},
	file = {Full Text:/Users/marcus/Zotero/storage/ENPRHTWL/Ouellet-Proulx et al. - 2017 - Water Temperature Ensemble Forecasts Implementati.pdf:application/pdf},
}


@article{stajkowski,
	title = {A {Methodology} for {Forecasting} {Dissolved} {Oxygen} in {Urban} {Streams}},
	volume = {12},
	issn = {2073-4441},
	url = {https://www.mdpi.com/2073-4441/12/9/2568},
	doi = {10.3390/w12092568},
	abstract = {Real-time monitoring of river water quality is at the forefront of a proactive urban water management strategy to meet the global challenge of vital freshwater resource sustainability. The concentration of dissolved oxygen (DO) is a primary indicator of the health state of the aquatic habitats, and its modeling is crucial for river water quality management. This paper investigates the importance of the choices of different techniques for preprocessing and stochastic modeling for developing a simple and reliable linear stochastic model for forecasting DO in urban rivers. We describe several methods of evaluation, preprocessing, and modeling for the DO parameter time series in the Credit River, Ontario, Canada, to achieve the optimum data preprocessing and input selection techniques and consequently obtain the optimum performance of the stochastic models as an effective river management tool. The Manly normalization and standardization (Std) methods were chosen for preprocessing the time series. Modeling the preprocessed time series using the stochastic autoregressive integrated moving average (ARIMA) model resulted in very accurate forecasts with a negligible difference from sole normalization and spectral analysis (Sf) methods.},
	language = {en},
	number = {9},
	urldate = {2024-02-15},
	journal = {Water},
	author = {Stajkowski, Stephen and Zeynoddin, Mohammad and Farghaly, Hani and Gharabaghi, Bahram and Bonakdari, Hossein},
	month = sep,
	year = {2020},
	pages = {2568},
	file = {Full Text:/Users/marcus/Zotero/storage/AFQSLVHH/Stajkowski et al. - 2020 - A Methodology for Forecasting Dissolved Oxygen in .pdf:application/pdf},
}


@article{chen_combining,
	title = {Combining physical-based model and machine learning to forecast chlorophyll-a concentration in freshwater lakes},
	volume = {907},
	issn = {00489697},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0048969723067244},
	doi = {10.1016/j.scitotenv.2023.168097},
	language = {en},
	urldate = {2024-02-15},
	journal = {Science of The Total Environment},
	author = {Chen, Cheng and Chen, Qiuwen and Yao, Siyang and He, Mengnan and Zhang, Jianyun and Li, Gang and Lin, Yuqing},
	month = jan,
	year = {2024},
	pages = {168097},
}


@article{maier,
	title = {Neural networks for the prediction and forecasting of water resources variables: a review of modelling issues and applications},
	volume = {15},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {13648152},
	shorttitle = {Neural networks for the prediction and forecasting of water resources variables},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815299000079},
	doi = {10.1016/S1364-8152(99)00007-9},
	language = {en},
	number = {1},
	urldate = {2024-05-13},
	journal = {Environmental Modelling \& Software},
	author = {Maier, Holger R. and Dandy, Graeme C.},
	month = jan,
	year = {2000},
	pages = {101--124},
	annote = {old review paper from 2000! wow
},
}


@article{hanson,
	title = {Predicting lake surface water phosphorus dynamics using process-guided machine learning},
	volume = {430},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380020302076},
	doi = {10.1016/j.ecolmodel.2020.109136},
	language = {en},
	urldate = {2024-02-13},
	journal = {Ecological Modelling},
	author = {Hanson, Paul C. and Stillman, Aviah B. and Jia, Xiaowei and Karpatne, Anuj and Dugan, Hilary A. and Carey, Cayelan C. and Stachelek, Jemma and Ward, Nicole K. and Zhang, Yu and Read, Jordan S. and Kumar, Vipin},
	month = aug,
	year = {2020},
	pages = {109136},
	file = {Full Text:/Users/marcus/Zotero/storage/A9MTE5DD/Hanson et al. - 2020 - Predicting lake surface water phosphorus dynamics .pdf:application/pdf},
}


@article{ladwig,
	title = {Modular {Compositional} {Learning} {Improves} {1D} {Hydrodynamic} {Lake} {Model} {Performance} by {Merging} {Process}‐{Based} {Modeling} {With} {Deep} {Learning}},
	volume = {16},
	issn = {1942-2466, 1942-2466},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023MS003953},
	doi = {10.1029/2023MS003953},
	language = {en},
	number = {1},
	urldate = {2024-06-19},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Ladwig, R. and Daw, A. and Albright, E. A. and Buelo, C. and Karpatne, A. and Meyer, M. F. and Neog, A. and Hanson, P. C. and Dugan, H. A.},
	month = jan,
	year = {2024},
	pages = {e2023MS003953},
	file = {Full Text:/Users/marcus/Zotero/storage/PB265UTV/Ladwig et al. - 2024 - Modular Compositional Learning Improves 1D Hydrody.pdf:application/pdf},
}


@article{zwart2023,
	title = {Evaluating deep learning architecture and data assimilation for improving water temperature forecasts at unmonitored locations},
	volume = {5},
	issn = {2624-9375},
	url = {https://www.frontiersin.org/articles/10.3389/frwa.2023.1184992/full},
	doi = {10.3389/frwa.2023.1184992},
	urldate = {2024-06-20},
	journal = {Frontiers in Water},
	author = {Zwart, Jacob A. and Diaz, Jeremy and Hamshaw, Scott and Oliver, Samantha and Ross, Jesse C. and Sleckman, Margaux and Appling, Alison P. and Corson-Dosch, Hayley and Jia, Xiaowei and Read, Jordan and Sadler, Jeffrey and Thompson, Theodore and Watkins, David and White, Elaheh},
	month = jun,
	year = {2023},
	pages = {1184992},
	file = {Full Text:/Users/marcus/Zotero/storage/MZHR8EJD/Zwart et al. - 2023 - Evaluating deep learning architecture and data ass.pdf:application/pdf},
}


@article{thomas_span_2023,
	title = {The {\textless}span style="font-variant:small-caps;"{\textgreater}{NEON}{\textless}/span{\textgreater} {Ecological} {Forecasting} {Challenge}},
	volume = {21},
	issn = {1540-9295, 1540-9309},
	shorttitle = {The {\textless}span style="font-variant},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/fee.2616},
	doi = {10.1002/fee.2616},
	language = {en},
	number = {3},
	urldate = {2024-06-20},
	journal = {Frontiers in Ecology and the Environment},
	author = {Thomas, R Quinn and Boettiger, Carl and Carey, Cayelan C and Dietze, Michael C and Johnson, Leah R and Kenney, Melissa A and McLachlan, Jason S and Peters, Jody A and Sokol, Eric R and Weltzin, Jake F and Willson, Alyssa and Woelmer, Whitney M and {Challenge contributors}},
	month = apr,
	year = {2023},
	pages = {112--113},
	file = {Full Text:/Users/marcus/Zotero/storage/BXTGNFGR/Thomas et al. - 2023 - The NEONs.pdf:application/pdf},
}


@article{wheeler_predicting_2024,
	title = {Predicting spring phenology in deciduous broadleaf forests: {NEON} phenology forecasting community challenge},
	volume = {345},
	issn = {01681923},
	shorttitle = {Predicting spring phenology in deciduous broadleaf forests},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168192323005002},
	doi = {10.1016/j.agrformet.2023.109810},
	language = {en},
	urldate = {2024-01-17},
	journal = {Agricultural and Forest Meteorology},
	author = {Wheeler, Kathryn I. and Dietze, Michael C. and LeBauer, David and Peters, Jody A. and Richardson, Andrew D. and Ross, Arun A. and Thomas, R. Quinn and Zhu, Kai and Bhat, Uttam and Munch, Stephan and Buzbee, Raphaela Floreani and Chen, Min and Goldstein, Benjamin and Guo, Jessica and Hao, Dalei and Jones, Chris and Kelly-Fair, Mira and Liu, Haoran and Malmborg, Charlotte and Neupane, Naresh and Pal, Debasmita and Shirey, Vaughn and Song, Yiluan and Steen, McKalee and Vance, Eric A. and Woelmer, Whitney M. and Wynne, Jacob H. and Zachmann, Luke},
	month = feb,
	year = {2024},
	pages = {109810},
}


@article{thomas_nearterm_2023,
	title = {Near‐term forecasts of {\textless}span style="font-variant:small-caps;"{\textgreater}{NEON}{\textless}/span{\textgreater} lakes reveal gradients of environmental predictability across the {\textless}span style="font-variant:small-caps;"{\textgreater}{US}{\textless}/span{\textgreater}},
	volume = {21},
	issn = {1540-9295, 1540-9309},
	shorttitle = {Near‐term forecasts of {\textless}span style="font-variant},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/fee.2623},
	doi = {10.1002/fee.2623},
	abstract = {The US National Ecological Observatory Network's (NEON's) standardized monitoring program provides an unprecedented opportunity for comparing the predictability of ecosystems. To harness the power of NEON data for examining environmental predictability, we scaled a near‐term, iterative, water temperature forecasting system to all six NEON lakes in the conterminous US. We generated 1‐day‐ahead to 35‐days‐ahead forecasts using a process‐based hydrodynamic model that was updated with observations as they became available. Among lakes, forecasts were more accurate than a null model up to 35‐days‐ahead, with an aggregated 1‐day‐ahead root‐mean square error (RMSE) of 0.61°C and a 35‐days‐ahead RMSE of 2.17°C. Water temperature forecast accuracy was positively associated with lake depth and water clarity, and negatively associated with fetch and catchment size. The results of our analysis suggest that lake characteristics interact with weather to control the predictability of thermal structure. Our work provides some of the first probabilistic forecasts of NEON sites and a framework for examining continental‐scale predictability.},
	language = {en},
	number = {5},
	urldate = {2024-01-17},
	journal = {Frontiers in Ecology and the Environment},
	author = {Thomas, R Quinn and McClure, Ryan P and Moore, Tadhg N and Woelmer, Whitney M and Boettiger, Carl and Figueiredo, Renato J and Hensley, Robert T and Carey, Cayelan C},
	month = jun,
	year = {2023},
	pages = {220--226},
	file = {Full Text:/Users/marcus/Zotero/storage/PNRGRNP4/Thomas et al. - 2023 - Near‐term forecasts of span style=font-variants.pdf:application/pdf},
}



@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2024-06-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}


@misc{lim_temporal_2019,
	title = {Temporal {Fusion} {Transformers} for {Interpretable} {Multi}-horizon {Time} {Series} {Forecasting}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1912.09363},
	doi = {10.48550/ARXIV.1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	year = {2019},
	note = {Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{oreshkin_n-beats_2019,
	title = {N-{BEATS}: {Neural} basis expansion analysis for interpretable time series forecasting},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {N-{BEATS}},
	url = {https://arxiv.org/abs/1905.10437},
	doi = {10.48550/ARXIV.1905.10437},
	abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
	year = {2019},
	note = {Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}


@article{assimakopoulos_theta_2000,
	title = {The theta model: a decomposition approach to forecasting},
	volume = {16},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01692070},
	shorttitle = {The theta model},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207000000662},
	doi = {10.1016/S0169-2070(00)00066-2},
	language = {en},
	number = {4},
	urldate = {2024-06-21},
	journal = {International Journal of Forecasting},
	author = {Assimakopoulos, V. and Nikolopoulos, K.},
	month = oct,
	year = {2000},
	pages = {521--530},
}


@article{makridakis_m3-competition_2000,
	title = {The {M3}-{Competition}: results, conclusions and implications},
	volume = {16},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01692070},
	shorttitle = {The {M3}-{Competition}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207000000571},
	doi = {10.1016/S0169-2070(00)00057-1},
	language = {en},
	number = {4},
	urldate = {2024-06-21},
	journal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Hibon, Michèle},
	month = oct,
	year = {2000},
	pages = {451--476},
}


@article{makridakis_m4_2020,
	title = {The {M4} {Competition}: 100,000 time series and 61 forecasting methods},
	volume = {36},
	issn = {01692070},
	shorttitle = {The {M4} {Competition}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301128},
	doi = {10.1016/j.ijforecast.2019.04.014},
	language = {en},
	number = {1},
	urldate = {2023-09-07},
	journal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	month = jan,
	year = {2020},
	pages = {54--74},
}


@article{fiorucci_models_2016,
	title = {Models for optimising the theta method and their relationship to state space models},
	volume = {32},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207016300243},
	doi = {10.1016/j.ijforecast.2016.02.005},
	language = {en},
	number = {4},
	urldate = {2024-05-24},
	journal = {International Journal of Forecasting},
	author = {Fiorucci, Jose A. and Pellegrini, Tiago R. and Louzada, Francisco and Petropoulos, Fotios and Koehler, Anne B.},
	month = oct,
	year = {2016},
	pages = {1151--1161},
	file = {Full Text:/Users/marcus/Zotero/storage/Z9P3QUIM/Fiorucci et al. - 2016 - Models for optimising the theta method and their r.pdf:application/pdf},
}


@article{hanin_universal_2019,
	title = {Universal {Function} {Approximation} by {Deep} {Neural} {Nets} with {Bounded} {Width} and {ReLU} {Activations}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/7/10/992},
	doi = {10.3390/math7100992},
	abstract = {This article concerns the expressive power of depth in neural nets with ReLU activations and a bounded width. We are particularly interested in the following questions: What is the minimal width      w min   ( d )      so that ReLU nets of width      w min   ( d )      (and arbitrary depth) can approximate any continuous function on the unit cube      [ 0 , 1 ]  d     arbitrarily well? For ReLU nets near this minimal width, what can one say about the depth necessary to approximate a given function? We obtain an essentially complete answer to these questions for convex functions. Our approach is based on the observation that, due to the convexity of the ReLU activation, ReLU nets are particularly well suited to represent convex functions. In particular, we prove that ReLU nets with width     d + 1     can approximate any continuous convex function of d variables arbitrarily well. These results then give quantitative depth estimates for the rate of approximation of any continuous scalar function on the d-dimensional cube      [ 0 , 1 ]  d     by ReLU nets with width     d + 3    .},
	language = {en},
	number = {10},
	urldate = {2024-06-21},
	journal = {Mathematics},
	author = {Hanin, Boris},
	month = oct,
	year = {2019},
	pages = {992},
	file = {Full Text:/Users/marcus/Zotero/storage/MWDQJCBD/Hanin - 2019 - Universal Function Approximation by Deep Neural Ne.pdf:application/pdf},
}


@article{adcock_gap_2020,
	title = {The gap between theory and practice in function approximation with deep neural networks},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2001.07523},
	doi = {10.48550/ARXIV.2001.07523},
	abstract = {Deep learning (DL) is transforming industry as decision-making processes are being automated by deep neural networks (DNNs) trained on real-world data. Driven partly by rapidly-expanding literature on DNN approximation theory showing they can approximate a rich variety of functions, such tools are increasingly being considered for problems in scientific computing. Yet, unlike traditional algorithms in this field, little is known about DNNs from the principles of numerical analysis, e.g., stability, accuracy, computational efficiency and sample complexity. In this paper we introduce a computational framework for examining DNNs in practice, and use it to study empirical performance with regard to these issues. We study performance of DNNs of different widths \&amp; depths on test functions in various dimensions, including smooth and piecewise smooth functions. We also compare DL against best-in-class methods for smooth function approx. based on compressed sensing (CS). Our main conclusion from these experiments is that there is a crucial gap between the approximation theory of DNNs and their practical performance, with trained DNNs performing relatively poorly on functions for which there are strong approximation results (e.g. smooth functions), yet performing well in comparison to best-in-class methods for other functions. To analyze this gap further, we provide some theoretical insights. We establish a practical existence theorem, asserting existence of a DNN architecture and training procedure that offers the same performance as CS. This establishes a key theoretical benchmark, showing the gap can be closed, albeit via a strategy guaranteed to perform as well as, but no better than, current best-in-class schemes. Nevertheless, it demonstrates the promise of practical DNN approx., by highlighting potential for better schemes through careful design of DNN architectures and training strategies.},
	urldate = {2024-02-23},
	author = {Adcock, Ben and Dexter, Nick},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, Numerical Analysis (math.NA)},
}



@misc{karpatne_knowledge-guided_2024,
	title = {Knowledge-guided {Machine} {Learning}: {Current} {Trends} and {Future} {Prospects}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Knowledge-guided {Machine} {Learning}},
	url = {https://arxiv.org/abs/2403.15989},
	doi = {10.48550/ARXIV.2403.15989},
	abstract = {This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Karpatne, Anuj and Jia, Xiaowei and Kumar, Vipin},
	year = {2024},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computational Engineering, Finance, and Science (cs.CE), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}


@article{zhi_deep_2024,
	title = {Deep learning for water quality},
	volume = {2},
	issn = {2731-6084},
	url = {https://www.nature.com/articles/s44221-024-00202-z},
	doi = {10.1038/s44221-024-00202-z},
	language = {en},
	number = {3},
	urldate = {2024-06-21},
	journal = {Nature Water},
	author = {Zhi, Wei and Appling, Alison P. and Golden, Heather E. and Podgorski, Joel and Li, Li},
	month = mar,
	year = {2024},
	pages = {228--241},
}


@article{herzen_darts_2021,
	title = {Darts: {User}-{Friendly} {Modern} {Machine} {Learning} for {Time} {Series}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Darts},
	url = {https://arxiv.org/abs/2110.03224},
	doi = {10.48550/ARXIV.2110.03224},
	abstract = {We present Darts, a Python machine learning library for time series, with a focus on forecasting. Darts offers a variety of models, from classics such as ARIMA to state-of-the-art deep neural networks. The emphasis of the library is on offering modern machine learning functionalities, such as supporting multidimensional series, meta-learning on multiple series, training on large datasets, incorporating external data, ensembling models, and providing a rich support for probabilistic forecasting. At the same time, great care goes into the API design to make it user-friendly and easy to use. For instance, all models can be used using fit()/predict(), similar to scikit-learn.},
	urldate = {2024-06-21},
	author = {Herzen, Julien and Lässig, Francesco and Piazzetta, Samuele Giuliano and Neuer, Thomas and Tafti, Léo and Raille, Guillaume and Van Pottelbergh, Tomas and Pasieka, Marek and Skrodzki, Andrzej and Huguenin, Nicolas and Dumonal, Maxime and Kościsz, Jan and Bader, Dennis and Gusset, Frédérick and Benheddi, Mounir and Williamson, Camila and Kosinski, Michal and Petrik, Matej and Grosch, Gaël},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Computation (stat.CO)},
	annote = {Other
Darts Github repository: https://github.com/unit8co/darts},
}


@article{olsson_multimodel_2024,
	title = {A {Multi}‐{Model} {Ensemble} of {Baseline} and {Process}‐{Based} {Models} {Improves} the {Predictive} {Skill} of {Near}‐{Term} {Lake} {Forecasts}},
	volume = {60},
	issn = {0043-1397, 1944-7973},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023WR035901},
	doi = {10.1029/2023WR035901},
	abstract = {Abstract
            Water temperature forecasting in lakes and reservoirs is a valuable tool to manage crucial freshwater resources in a changing and more variable climate, but previous efforts have yet to identify an optimal modeling approach. Here, we demonstrate the first multi‐model ensemble (MME) reservoir water temperature forecast, a forecasting method that combines individual model strengths in a single forecasting framework. We developed two MMEs: a three‐model process‐based MME and a five‐model MME that includes process‐based and empirical models to forecast water temperature profiles at a temperate drinking water reservoir. We found that the five‐model MME improved forecast performance by 8\%–30\% relative to individual models and the process‐based MME, as quantified using an aggregated probabilistic skill score. This increase in performance was due to large improvements in forecast bias in the five‐model MME, despite increases in forecast uncertainty. High correlation among the process‐based models resulted in little improvement in forecast performance in the process‐based MME relative to the individual process‐based models. The utility of MMEs is highlighted by two results: (a) no individual model performed best at every depth and horizon (days in the future), and (b) MMEs avoided poor performances by rarely producing the worst forecast for any single forecasted period ({\textless}6\% of the worst ranked forecasts over time). This work presents an example of how existing models can be combined to improve water temperature forecasting in lakes and reservoirs and discusses the value of utilizing MMEs, rather than individual models, in operational forecasts.
          , 
            Key Points
            
              
                
                  Aggregated lake temperature forecast skill was higher for multi‐model ensemble (MME) forecasts than individual model forecasts
                
                
                  Including baseline empirical models (day‐of‐year, persistence) with process models improved MME forecast performance
                
                
                  MME forecasts improved forecast skill by “hedging,” as no individual model performed best at all horizons or depths},
	language = {en},
	number = {3},
	urldate = {2024-06-21},
	journal = {Water Resources Research},
	author = {Olsson, Freya and Moore, Tadhg N. and Carey, Cayelan C. and Breef‐Pilz, Adrienne and Thomas, R. Quinn},
	month = mar,
	year = {2024},
	pages = {e2023WR035901},
}


@article{read_processguided_2019,
	title = {Process‐{Guided} {Deep} {Learning} {Predictions} of {Lake} {Water} {Temperature}},
	volume = {55},
	issn = {0043-1397, 1944-7973},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019WR024922},
	doi = {10.1029/2019WR024922},
	abstract = {Abstract
            The rapid growth of data in water resources has created new opportunities to accelerate knowledge discovery with the use of advanced deep learning tools. Hybrid models that integrate theory with state‐of‐the art empirical techniques have the potential to improve predictions while remaining true to physical laws. This paper evaluates the Process‐Guided Deep Learning (PGDL) hybrid modeling framework with a use‐case of predicting depth‐specific lake water temperatures. The PGDL model has three primary components: a deep learning model with temporal awareness (long short‐term memory recurrence), theory‐based feedback (model penalties for violating conversation of energy), and model pretraining to initialize the network with synthetic data (water temperature predictions from a process‐based model). In situ water temperatures were used to train the PGDL model, a deep learning (DL) model, and a process‐based (PB) model. Model performance was evaluated in various conditions, including when training data were sparse and when predictions were made outside of the range in the training data set. The PGDL model performance (as measured by root‐mean‐square error (RMSE)) was superior to DL and PB for two detailed study lakes, but only when pretraining data included greater variability than the training period. The PGDL model also performed well when extended to 68 lakes, with a median RMSE of 1.65 °C during the test period (DL: 1.78 °C, PB: 2.03 °C; in a small number of lakes PB or DL models were more accurate). This case‐study demonstrates that integrating scientific knowledge into deep learning tools shows promise for improving predictions of many important environmental variables.
          , 
            Key Points
            
              
                
                  Process‐Guided Deep Learning (PGDL) models integrate advanced empirical techniques with process knowledge
                
                
                  We used PGDL to accurately predict lake water temperatures for various conditions
                
                
                  PGDL performance improved significantly when pretraining data included diverse conditions generated by an existing process‐based model},
	language = {en},
	number = {11},
	urldate = {2024-06-21},
	journal = {Water Resources Research},
	author = {Read, Jordan S. and Jia, Xiaowei and Willard, Jared and Appling, Alison P. and Zwart, Jacob A. and Oliver, Samantha K. and Karpatne, Anuj and Hansen, Gretchen J. A. and Hanson, Paul C. and Watkins, William and Steinbach, Michael and Kumar, Vipin},
	month = nov,
	year = {2019},
	pages = {9173--9190},
}


@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2024-06-22},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Van Den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
}


@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Abstract
            
              Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort
              1–4
              , the structures of around 100,000 unique proteins have been determined
              5
              , but this represents a small fraction of the billions of known protein sequences
              6,7
              . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’
              8
              —has been an important open research problem for more than 50 years
              9
              . Despite recent progress
              10–14
              , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)
              15
              , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2024-06-22},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	pages = {583--589},
	file = {Full Text:/Users/marcus/Zotero/storage/9AYCEKW9/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf:application/pdf},
}


@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.18223},
	doi = {10.48550/ARXIV.2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2024-06-22},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	year = {2023},
	note = {Version Number: 13},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI)},
	annote = {Other
ongoing work; 124 pages, 946 citations},
}


@misc{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1803.01271},
	doi = {10.48550/ARXIV.1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2018},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}


@misc{zeng_are_2022,
	title = {Are {Transformers} {Effective} for {Time} {Series} {Forecasting}?},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2205.13504},
	doi = {10.48550/ARXIV.2205.13504},
	abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the {\textbackslash}emph\{permutation-invariant\} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: {\textbackslash}url\{https://github.com/cure-lab/LTSF-Linear\}.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
	year = {2022},
	note = {Version Number: 3},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {Other
Code is available at https://github.com/cure-lab/LTSF-Linear},
}


@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1706.03762},
	doi = {10.48550/ARXIV.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	note = {Version Number: 7},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	annote = {Other
15 pages, 5 figures},
}


@article{du_multivariate_2020,
	title = {Multivariate time series forecasting via attention-based encoder–decoder framework},
	volume = {388},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220300606},
	doi = {10.1016/j.neucom.2019.12.118},
	language = {en},
	urldate = {2024-06-27},
	journal = {Neurocomputing},
	author = {Du, Shengdong and Li, Tianrui and Yang, Yan and Horng, Shi-Jinn},
	month = may,
	year = {2020},
	pages = {269--279},
}
