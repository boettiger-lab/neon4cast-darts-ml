
@article{albert2021,
	title = {Scientists’ warning to humanity on the freshwater biodiversity crisis},
	volume = {50},
	issn = {0044-7447, 1654-7209},
	url = {http://link.springer.com/10.1007/s13280-020-01318-8},
	doi = {10.1007/s13280-020-01318-8},
	language = {en},
	number = {1},
	urldate = {2024-05-21},
	journal = {Ambio},
	author = {Albert, James S. and Destouni, Georgia and Duke-Sylvester, Scott M. and Magurran, Anne E. and Oberdorff, Thierry and Reis, Roberto E. and Winemiller, Kirk O. and Ripple, William J.},
	month = jan,
	year = {2021},
	pages = {85--94},
	file = {Full Text:/Users/marcus/Zotero/storage/RWHACJ3X/Albert et al. - 2021 - Scientists’ warning to humanity on the freshwater .pdf:application/pdf},
}


@article{ouellet-proulx,
	title = {Water {Temperature} {Ensemble} {Forecasts}: {Implementation} {Using} the {CEQUEAU} {Model} on {Two} {Contrasted} {River} {Systems}},
	volume = {9},
	issn = {2073-4441},
	shorttitle = {Water {Temperature} {Ensemble} {Forecasts}},
	url = {http://www.mdpi.com/2073-4441/9/7/457},
	doi = {10.3390/w9070457},
	language = {en},
	number = {7},
	urldate = {2024-02-15},
	journal = {Water},
	author = {Ouellet-Proulx, Sébastien and St-Hilaire, André and Boucher, Marie-Amélie},
	month = jun,
	year = {2017},
	pages = {457},
	file = {Full Text:/Users/marcus/Zotero/storage/ENPRHTWL/Ouellet-Proulx et al. - 2017 - Water Temperature Ensemble Forecasts Implementati.pdf:application/pdf},
}


@article{stajkowski,
	title = {A {Methodology} for {Forecasting} {Dissolved} {Oxygen} in {Urban} {Streams}},
	volume = {12},
	issn = {2073-4441},
	url = {https://www.mdpi.com/2073-4441/12/9/2568},
	doi = {10.3390/w12092568},
	abstract = {Real-time monitoring of river water quality is at the forefront of a proactive urban water management strategy to meet the global challenge of vital freshwater resource sustainability. The concentration of dissolved oxygen (DO) is a primary indicator of the health state of the aquatic habitats, and its modeling is crucial for river water quality management. This paper investigates the importance of the choices of different techniques for preprocessing and stochastic modeling for developing a simple and reliable linear stochastic model for forecasting DO in urban rivers. We describe several methods of evaluation, preprocessing, and modeling for the DO parameter time series in the Credit River, Ontario, Canada, to achieve the optimum data preprocessing and input selection techniques and consequently obtain the optimum performance of the stochastic models as an effective river management tool. The Manly normalization and standardization (Std) methods were chosen for preprocessing the time series. Modeling the preprocessed time series using the stochastic autoregressive integrated moving average (ARIMA) model resulted in very accurate forecasts with a negligible difference from sole normalization and spectral analysis (Sf) methods.},
	language = {en},
	number = {9},
	urldate = {2024-02-15},
	journal = {Water},
	author = {Stajkowski, Stephen and Zeynoddin, Mohammad and Farghaly, Hani and Gharabaghi, Bahram and Bonakdari, Hossein},
	month = sep,
	year = {2020},
	pages = {2568},
	file = {Full Text:/Users/marcus/Zotero/storage/AFQSLVHH/Stajkowski et al. - 2020 - A Methodology for Forecasting Dissolved Oxygen in .pdf:application/pdf},
}


@article{chen_combining,
	title = {Combining physical-based model and machine learning to forecast chlorophyll-a concentration in freshwater lakes},
	volume = {907},
	issn = {00489697},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0048969723067244},
	doi = {10.1016/j.scitotenv.2023.168097},
	language = {en},
	urldate = {2024-02-15},
	journal = {Science of The Total Environment},
	author = {Chen, Cheng and Chen, Qiuwen and Yao, Siyang and He, Mengnan and Zhang, Jianyun and Li, Gang and Lin, Yuqing},
	month = jan,
	year = {2024},
	pages = {168097},
}


@article{maier,
	title = {Neural networks for the prediction and forecasting of water resources variables: a review of modelling issues and applications},
	volume = {15},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {13648152},
	shorttitle = {Neural networks for the prediction and forecasting of water resources variables},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364815299000079},
	doi = {10.1016/S1364-8152(99)00007-9},
	language = {en},
	number = {1},
	urldate = {2024-05-13},
	journal = {Environmental Modelling \& Software},
	author = {Maier, Holger R. and Dandy, Graeme C.},
	month = jan,
	year = {2000},
	pages = {101--124},
	annote = {old review paper from 2000! wow
},
}


@article{hanson,
	title = {Predicting lake surface water phosphorus dynamics using process-guided machine learning},
	volume = {430},
	issn = {03043800},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304380020302076},
	doi = {10.1016/j.ecolmodel.2020.109136},
	language = {en},
	urldate = {2024-02-13},
	journal = {Ecological Modelling},
	author = {Hanson, Paul C. and Stillman, Aviah B. and Jia, Xiaowei and Karpatne, Anuj and Dugan, Hilary A. and Carey, Cayelan C. and Stachelek, Jemma and Ward, Nicole K. and Zhang, Yu and Read, Jordan S. and Kumar, Vipin},
	month = aug,
	year = {2020},
	pages = {109136},
	file = {Full Text:/Users/marcus/Zotero/storage/A9MTE5DD/Hanson et al. - 2020 - Predicting lake surface water phosphorus dynamics .pdf:application/pdf},
}


@article{ladwig,
	title = {Modular {Compositional} {Learning} {Improves} {1D} {Hydrodynamic} {Lake} {Model} {Performance} by {Merging} {Process}‐{Based} {Modeling} {With} {Deep} {Learning}},
	volume = {16},
	issn = {1942-2466, 1942-2466},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023MS003953},
	doi = {10.1029/2023MS003953},
	language = {en},
	number = {1},
	urldate = {2024-06-19},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Ladwig, R. and Daw, A. and Albright, E. A. and Buelo, C. and Karpatne, A. and Meyer, M. F. and Neog, A. and Hanson, P. C. and Dugan, H. A.},
	month = jan,
	year = {2024},
	pages = {e2023MS003953},
	file = {Full Text:/Users/marcus/Zotero/storage/PB265UTV/Ladwig et al. - 2024 - Modular Compositional Learning Improves 1D Hydrody.pdf:application/pdf},
}


@article{zwart2023,
	title = {Evaluating deep learning architecture and data assimilation for improving water temperature forecasts at unmonitored locations},
	volume = {5},
	issn = {2624-9375},
	url = {https://www.frontiersin.org/articles/10.3389/frwa.2023.1184992/full},
	doi = {10.3389/frwa.2023.1184992},
	urldate = {2024-06-20},
	journal = {Frontiers in Water},
	author = {Zwart, Jacob A. and Diaz, Jeremy and Hamshaw, Scott and Oliver, Samantha and Ross, Jesse C. and Sleckman, Margaux and Appling, Alison P. and Corson-Dosch, Hayley and Jia, Xiaowei and Read, Jordan and Sadler, Jeffrey and Thompson, Theodore and Watkins, David and White, Elaheh},
	month = jun,
	year = {2023},
	pages = {1184992},
	file = {Full Text:/Users/marcus/Zotero/storage/MZHR8EJD/Zwart et al. - 2023 - Evaluating deep learning architecture and data ass.pdf:application/pdf},
}


@article{thomas_span_2023,
	title = {The {\textless}span style="font-variant:small-caps;"{\textgreater}{NEON}{\textless}/span{\textgreater} {Ecological} {Forecasting} {Challenge}},
	volume = {21},
	issn = {1540-9295, 1540-9309},
	shorttitle = {The {\textless}span style="font-variant},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/fee.2616},
	doi = {10.1002/fee.2616},
	language = {en},
	number = {3},
	urldate = {2024-06-20},
	journal = {Frontiers in Ecology and the Environment},
	author = {Thomas, R Quinn and Boettiger, Carl and Carey, Cayelan C and Dietze, Michael C and Johnson, Leah R and Kenney, Melissa A and McLachlan, Jason S and Peters, Jody A and Sokol, Eric R and Weltzin, Jake F and Willson, Alyssa and Woelmer, Whitney M and {Challenge contributors}},
	month = apr,
	year = {2023},
	pages = {112--113},
	file = {Full Text:/Users/marcus/Zotero/storage/BXTGNFGR/Thomas et al. - 2023 - The NEONs.pdf:application/pdf},
}


@article{wheeler_predicting_2024,
	title = {Predicting spring phenology in deciduous broadleaf forests: {NEON} phenology forecasting community challenge},
	volume = {345},
	issn = {01681923},
	shorttitle = {Predicting spring phenology in deciduous broadleaf forests},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168192323005002},
	doi = {10.1016/j.agrformet.2023.109810},
	language = {en},
	urldate = {2024-01-17},
	journal = {Agricultural and Forest Meteorology},
	author = {Wheeler, Kathryn I. and Dietze, Michael C. and LeBauer, David and Peters, Jody A. and Richardson, Andrew D. and Ross, Arun A. and Thomas, R. Quinn and Zhu, Kai and Bhat, Uttam and Munch, Stephan and Buzbee, Raphaela Floreani and Chen, Min and Goldstein, Benjamin and Guo, Jessica and Hao, Dalei and Jones, Chris and Kelly-Fair, Mira and Liu, Haoran and Malmborg, Charlotte and Neupane, Naresh and Pal, Debasmita and Shirey, Vaughn and Song, Yiluan and Steen, McKalee and Vance, Eric A. and Woelmer, Whitney M. and Wynne, Jacob H. and Zachmann, Luke},
	month = feb,
	year = {2024},
	pages = {109810},
}


@article{thomas_nearterm_2023,
	title = {Near‐term forecasts of {\textless}span style="font-variant:small-caps;"{\textgreater}{NEON}{\textless}/span{\textgreater} lakes reveal gradients of environmental predictability across the {\textless}span style="font-variant:small-caps;"{\textgreater}{US}{\textless}/span{\textgreater}},
	volume = {21},
	issn = {1540-9295, 1540-9309},
	shorttitle = {Near‐term forecasts of {\textless}span style="font-variant},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/10.1002/fee.2623},
	doi = {10.1002/fee.2623},
	abstract = {The US National Ecological Observatory Network's (NEON's) standardized monitoring program provides an unprecedented opportunity for comparing the predictability of ecosystems. To harness the power of NEON data for examining environmental predictability, we scaled a near‐term, iterative, water temperature forecasting system to all six NEON lakes in the conterminous US. We generated 1‐day‐ahead to 35‐days‐ahead forecasts using a process‐based hydrodynamic model that was updated with observations as they became available. Among lakes, forecasts were more accurate than a null model up to 35‐days‐ahead, with an aggregated 1‐day‐ahead root‐mean square error (RMSE) of 0.61°C and a 35‐days‐ahead RMSE of 2.17°C. Water temperature forecast accuracy was positively associated with lake depth and water clarity, and negatively associated with fetch and catchment size. The results of our analysis suggest that lake characteristics interact with weather to control the predictability of thermal structure. Our work provides some of the first probabilistic forecasts of NEON sites and a framework for examining continental‐scale predictability.},
	language = {en},
	number = {5},
	urldate = {2024-01-17},
	journal = {Frontiers in Ecology and the Environment},
	author = {Thomas, R Quinn and McClure, Ryan P and Moore, Tadhg N and Woelmer, Whitney M and Boettiger, Carl and Figueiredo, Renato J and Hensley, Robert T and Carey, Cayelan C},
	month = jun,
	year = {2023},
	pages = {220--226},
	file = {Full Text:/Users/marcus/Zotero/storage/PNRGRNP4/Thomas et al. - 2023 - Near‐term forecasts of span style=font-variants.pdf:application/pdf},
}



@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2024-06-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}


@misc{lim_temporal_2019,
	title = {Temporal {Fusion} {Transformers} for {Interpretable} {Multi}-horizon {Time} {Series} {Forecasting}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1912.09363},
	doi = {10.48550/ARXIV.1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	year = {2019},
	note = {Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}


@misc{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1803.01271},
	doi = {10.48550/ARXIV.1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	year = {2018},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Artificial Intelligence (cs.AI)},
}


@misc{oreshkin_n-beats_2019,
	title = {N-{BEATS}: {Neural} basis expansion analysis for interpretable time series forecasting},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {N-{BEATS}},
	url = {https://arxiv.org/abs/1905.10437},
	doi = {10.48550/ARXIV.1905.10437},
	abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
	year = {2019},
	note = {Version Number: 4},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}


@article{assimakopoulos_theta_2000,
	title = {The theta model: a decomposition approach to forecasting},
	volume = {16},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01692070},
	shorttitle = {The theta model},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207000000662},
	doi = {10.1016/S0169-2070(00)00066-2},
	language = {en},
	number = {4},
	urldate = {2024-06-21},
	journal = {International Journal of Forecasting},
	author = {Assimakopoulos, V. and Nikolopoulos, K.},
	month = oct,
	year = {2000},
	pages = {521--530},
}


@article{makridakis_m3-competition_2000,
	title = {The {M3}-{Competition}: results, conclusions and implications},
	volume = {16},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01692070},
	shorttitle = {The {M3}-{Competition}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207000000571},
	doi = {10.1016/S0169-2070(00)00057-1},
	language = {en},
	number = {4},
	urldate = {2024-06-21},
	journal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Hibon, Michèle},
	month = oct,
	year = {2000},
	pages = {451--476},
}


@article{makridakis_m4_2020,
	title = {The {M4} {Competition}: 100,000 time series and 61 forecasting methods},
	volume = {36},
	issn = {01692070},
	shorttitle = {The {M4} {Competition}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207019301128},
	doi = {10.1016/j.ijforecast.2019.04.014},
	language = {en},
	number = {1},
	urldate = {2023-09-07},
	journal = {International Journal of Forecasting},
	author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
	month = jan,
	year = {2020},
	pages = {54--74},
}


@article{fiorucci_models_2016,
	title = {Models for optimising the theta method and their relationship to state space models},
	volume = {32},
	issn = {01692070},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169207016300243},
	doi = {10.1016/j.ijforecast.2016.02.005},
	language = {en},
	number = {4},
	urldate = {2024-05-24},
	journal = {International Journal of Forecasting},
	author = {Fiorucci, Jose A. and Pellegrini, Tiago R. and Louzada, Francisco and Petropoulos, Fotios and Koehler, Anne B.},
	month = oct,
	year = {2016},
	pages = {1151--1161},
	file = {Full Text:/Users/marcus/Zotero/storage/Z9P3QUIM/Fiorucci et al. - 2016 - Models for optimising the theta method and their r.pdf:application/pdf},
}


@article{hanin_universal_2019,
	title = {Universal {Function} {Approximation} by {Deep} {Neural} {Nets} with {Bounded} {Width} and {ReLU} {Activations}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/7/10/992},
	doi = {10.3390/math7100992},
	abstract = {This article concerns the expressive power of depth in neural nets with ReLU activations and a bounded width. We are particularly interested in the following questions: What is the minimal width      w min   ( d )      so that ReLU nets of width      w min   ( d )      (and arbitrary depth) can approximate any continuous function on the unit cube      [ 0 , 1 ]  d     arbitrarily well? For ReLU nets near this minimal width, what can one say about the depth necessary to approximate a given function? We obtain an essentially complete answer to these questions for convex functions. Our approach is based on the observation that, due to the convexity of the ReLU activation, ReLU nets are particularly well suited to represent convex functions. In particular, we prove that ReLU nets with width     d + 1     can approximate any continuous convex function of d variables arbitrarily well. These results then give quantitative depth estimates for the rate of approximation of any continuous scalar function on the d-dimensional cube      [ 0 , 1 ]  d     by ReLU nets with width     d + 3    .},
	language = {en},
	number = {10},
	urldate = {2024-06-21},
	journal = {Mathematics},
	author = {Hanin, Boris},
	month = oct,
	year = {2019},
	pages = {992},
	file = {Full Text:/Users/marcus/Zotero/storage/MWDQJCBD/Hanin - 2019 - Universal Function Approximation by Deep Neural Ne.pdf:application/pdf},
}


@article{adcock_gap_2020,
	title = {The gap between theory and practice in function approximation with deep neural networks},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2001.07523},
	doi = {10.48550/ARXIV.2001.07523},
	abstract = {Deep learning (DL) is transforming industry as decision-making processes are being automated by deep neural networks (DNNs) trained on real-world data. Driven partly by rapidly-expanding literature on DNN approximation theory showing they can approximate a rich variety of functions, such tools are increasingly being considered for problems in scientific computing. Yet, unlike traditional algorithms in this field, little is known about DNNs from the principles of numerical analysis, e.g., stability, accuracy, computational efficiency and sample complexity. In this paper we introduce a computational framework for examining DNNs in practice, and use it to study empirical performance with regard to these issues. We study performance of DNNs of different widths \&amp; depths on test functions in various dimensions, including smooth and piecewise smooth functions. We also compare DL against best-in-class methods for smooth function approx. based on compressed sensing (CS). Our main conclusion from these experiments is that there is a crucial gap between the approximation theory of DNNs and their practical performance, with trained DNNs performing relatively poorly on functions for which there are strong approximation results (e.g. smooth functions), yet performing well in comparison to best-in-class methods for other functions. To analyze this gap further, we provide some theoretical insights. We establish a practical existence theorem, asserting existence of a DNN architecture and training procedure that offers the same performance as CS. This establishes a key theoretical benchmark, showing the gap can be closed, albeit via a strategy guaranteed to perform as well as, but no better than, current best-in-class schemes. Nevertheless, it demonstrates the promise of practical DNN approx., by highlighting potential for better schemes through careful design of DNN architectures and training strategies.},
	urldate = {2024-02-23},
	author = {Adcock, Ben and Dexter, Nick},
	year = {2020},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, Numerical Analysis (math.NA)},
}


